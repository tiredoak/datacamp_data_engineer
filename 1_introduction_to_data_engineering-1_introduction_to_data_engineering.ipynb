{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Data Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data engineer is one that develops, constructs, tests, and maintains architectures such as databases and large-scale processing systems.\n",
    "\n",
    "This usually involves:\n",
    "- processing large amounts of data\n",
    "- (consequently) using clusters of machines\n",
    "\n",
    "Some typical data engineer tasks:\n",
    "- develop scalable data architecture\n",
    "- streamline data acquisition\n",
    "- set up processes to bring together data\n",
    "- clean corrupt data\n",
    "- using cloud technology\n",
    "\n",
    "## Tools of the data engineer\n",
    "\n",
    "A data engineer's tasks begin and end with databases. Within a business we  typically have (1) databases which support the application and (2) databases which are used for analysis.\n",
    "\n",
    "Some typical tasks are:\n",
    "1. Processing (cleaning, aggregating and joining data - since this usually means large amounts of data, it is intrinsically tied to parallel processing frameworks)\n",
    "2. Scheduling (planning jobs with specific intervals, resolving dependency of jobs)\n",
    "\n",
    "## Examples of tools\n",
    "\n",
    "- Databases: MySQL, PostgreSQL\n",
    "- Processing: Apache Spark, Hive\n",
    "- Scheduling: Airflow, Oozie, Luigi\n",
    "\n",
    "## Cloud Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a company to hold their own servers is extremely unpractical: there's many associated costs that come with it such as maintenance, cost of insurance, unused downtime, etc.\n",
    "\n",
    "It is much more practical to outsource all of this to a company (a cloud provider) who is responsible for all these costs.\n",
    "\n",
    "Typically cloud providers offer: storage, computation and databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A database is a usually large collection of data organized especially for rapid search and retrieval. It holds, organizes and searches/retrieves data (through a database management system, or DBMS).\n",
    "\n",
    "If differs from a filesystem in that it is very organized and includes and is optimised for functionalites such as search, replication, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "Data can generally be structured or unstructured (or in between). Structured data is data with a schema whereas unstructured data could be a video or a photo. In between we have *semi-structured data* such as JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and NoSQL\n",
    "\n",
    "We can have either SQL or NoSQL structures.\n",
    "\n",
    "They differ in a few key points. SQL data is represented by **tables**, with a database schema. They are based in relational databases. \n",
    "\n",
    "NoSQL can is always non-relational and can be both structured or unstructured. Two popular types of NoSQL databases are key-value stores (where Redis is the most popular) or document stores (where MongoDB is the most popular choice).\n",
    "\n",
    "In key-value stores the values are usually simple: use-cases are caching or distributed configuration, for instance.\n",
    "\n",
    "In document stores, values are usually semi-structured objects like a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schema in SQL\n",
    "\n",
    "A popular schema is the star schema. This consists of one or more fact tables which reference any number of dimension tables. In this schema, facts are things that happened (e.g. product orders) and dimensions is information about the world (e.g. customer information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating two tables in SQL\n",
    "CREATE TABLE \"Customer\" (\n",
    "    \"id\" SERIAL NOT NULL,\n",
    "    \"first_name\" varchar,\n",
    "    \"last_name\" varchar,\n",
    "    PRIMARY KEY (\"id\")\n",
    ");\n",
    "\n",
    "CREATE TABLE \"Order\" (\n",
    "    \"id\" SERIAL NOT NULL,\n",
    "    \"customer_id\" integer REFERENCES \"Customer\",\n",
    "    \"product_name\" varchar,\n",
    "    \"product_price\" integer,\n",
    "    PRIMARY KEY (\"id\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star Schema\n",
    "\n",
    "Common database schema which consists of one or more **fact tables** which reference n **dimension tables**\n",
    "\n",
    "- **Facts**: things that happened, such as product orders\n",
    "- **Dimensions**: information on the world (e.g. customer information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel computing is fundamental because of **memory** and **processing power**. The amounts of data used usually exceed what a single computer can hold in memory. With parallel computing, we distribute the operations we are working on (e.g. cleaning or transforming data) across a cluster of machines which each execute a portion of the task. The result is then combined.\n",
    "\n",
    "Pros:\n",
    "- processing power;\n",
    "- we can partition the dataset;\n",
    "\n",
    "Cons:\n",
    "- task needs to be large;\n",
    "- communication overhead;\n",
    "- parallel slowdown (the speed does not increase linearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_mean_age(year_and_group):\n",
    "    year, group = year_and_group\n",
    "    return pd.DataFrame({\"Age\": group[\"Age\".mean()]}, index=[year])\n",
    "\n",
    "with Pool(4) as p:\n",
    "    # apply our function across the groups of a dataframe\n",
    "    results = p.map(take_mean_age, athlete_events.groupby(\"Year\"))\n",
    "    \n",
    "result_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# partition dataframe into 4\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions=4)\n",
    "\n",
    "# run parallel computation on each partition\n",
    "result_df = athlete_events_dask.groupby(\"Year\").Age.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computing Frameworks\n",
    "\n",
    "### Background \n",
    "\n",
    "Apache Hadoop is a collection of open-source projects maintained by the Apache Software Foundation. The two important ones to discuss initially are **MapReduce** and **HDFS**. \n",
    "\n",
    "- **HDFS** is a distributed file system (similar to a normal file system, only files are distributed across multiple machines). It is often replaced now by S3 or Cloud Storage\n",
    "- **MapReduce** is a Big Data processing paradigm which splits computation across a cluster of machines. The problem with MapReduce was the complexity in writing - this is what led to Hive\n",
    "- **Hive** is a layer on top of Hadoop ecosystem that makes data from several sources queryable using a Structured Query Language, Hive SQL. MapReduce was the implmeentaion originally but now it works across a bunch of tools. It was initially developed by Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive SQL sample\n",
    "SELECT year, AVG(age)\n",
    "FROM views.athlete_events\n",
    "GROUP BY year\n",
    "\n",
    "# looks like re0gular SQL but under the hood the query is transformed into a job which can run across a cluster of\n",
    "# computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Spark** distributes data over a cluster of computers. In MapReduce-based systems tend to need expensive disk writes between jobs, Spark tries to keep as much as possible in memory. \n",
    "    - It relies on Resilient Distributed Datasets or RDDs.\n",
    "    - RDDs are conceptually like a list of tuples\n",
    "    - We can apply two types of operations on RDDs: **transformations** (`.map()` or  `.filter()`) or **actions** (`.count()` or `.first()`)\n",
    "    - **Transformations** results in transformed RDDs while **actions** result in a single value\n",
    "    - When working with RDDs we tipically used a programming language interface like PySpark\n",
    "        - PySpark uses a DataFrame abstraction\n",
    "        - Looks similar to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming existence of athlete_events_spark, a Spark DataFrame\n",
    "\n",
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean(\"Age\"))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean(\"Age\").show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running PySpark locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## /home/repl/spark-script.py\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    athlete_events_spark = (spark\n",
    "        .read\n",
    "        .csv(\"/home/repl/datasets/athlete_events.csv\",\n",
    "             header=True,\n",
    "             inferSchema=True,\n",
    "             escape='\"'))\n",
    "\n",
    "    athlete_events_spark = (athlete_events_spark\n",
    "        .withColumn(\"Height\",\n",
    "                    athlete_events_spark.Height.cast(\"integer\")))\n",
    "\n",
    "    print(athlete_events_spark\n",
    "        .groupBy('Year')\n",
    "        .mean('Height')\n",
    "        .orderBy('Year')\n",
    "        .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master local[4] /home/repl/spark-script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Scheduling Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a simple Spark Job which:\n",
    "- pulls data from a CSV file;\n",
    "- filters out some corrupt records;\n",
    "- loads data into a SQL database, ready for analysis\n",
    "\n",
    "We need a scheduler to ensure this flow works properly and in the right dependencies (CRON is insufficient because it does not express precedence of tasks). \n",
    "\n",
    "This is usually achieved by a Directed Acyclic Graph (DAG) where each node is a task and the edges connecting them represent the order in which the process should run.\n",
    "\n",
    "### Example frameworks\n",
    " \n",
    "- Luigi (Spotify)\n",
    "- Airflow (Airbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(\n",
    "    dag_id=\"car_factory_simulation\",\n",
    "    default_args={\n",
    "        \"owner\": \"airflow\",\n",
    "        \"start_date\": airflow.utils.dates.days_ago(2)\n",
    "    },\n",
    "    schedule_interval=\"0 * * * *\"\n",
    ")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "## Extract\n",
    "\n",
    "### Files\n",
    "\n",
    "Data processing tasks start which extracting some data from a source. Sources can be split into:\n",
    "- unstructured data (plain text)\n",
    "- flat files (rows are records and columns are attributes, like a `.csv`)\n",
    "- semi-structured (e.g. JSON)\n",
    "\n",
    "JSON consists of four atomic data types:\n",
    "1. number\n",
    "2. string\n",
    "3. boolean\n",
    "4. null\n",
    "\n",
    "and two composite data types:\n",
    "1. array\n",
    "2. object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n",
    "\n",
    "Data moves on the Web through **requests**. When we go to a website we request data which is returned for us by a server and rendered in our browser. Some servers return data to be read by humans and some return data which is destined to be used in programs. These are APIs and they typically return data in a  JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'neis', 'descendants': 0, 'id': 16222426, 'score': 17, 'time': 1516800333, 'title': 'Duolingo-Style Learning for Data Science: DataCamp for Mobile', 'type': 'story', 'url': 'https://medium.com/datacamp/duolingo-style-learning-for-data-science-datacamp-for-mobile-3861d1bc02df'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases\n",
    "\n",
    "Most common way to extract data. They can be either:\n",
    "\n",
    "- Applications databases:\n",
    "    - Transactions\n",
    "    - Inserts or changes\n",
    "    - OLTP (online transaction processing)\n",
    "    - Row-oriented\n",
    "- Analytical databases:\n",
    "    - OLAP (online analytical processing)\n",
    "    - Column-oriented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to a database we **always need a connection string (or URI)**. This looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql://[user[:password]@[host][:port]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_sql(\"SELECT * FROM customer\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n",
    "\n",
    "A typical transformation in Python would be to split a string column containing a customer's name into first and last name, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rental rate column as a string\n",
    "rental_rate_str = film_df.rental_rate.astype(\"str\")\n",
    "\n",
    "# Split up and expand the column\n",
    "rental_rate_expanded = rental_rate_str.str.split(\".\", expand=True)\n",
    "\n",
    "# Assign the columns to film_df\n",
    "film_df = film_df.assign(\n",
    "    rental_rate_dollar=rental_rate_expanded[0],\n",
    "    rental_rate_cents=rental_rate_expanded[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first read from a database\n",
    "spark.read.jdbc(\n",
    "    \"jdbc:postgresql://localhost:5432/pagila\",\n",
    "    \"customer\",\n",
    "    {\n",
    "        \"user\":\"repl\",\n",
    "        \"password\":\"password\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupBy and mean to aggregate the column\n",
    "ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')\n",
    "\n",
    "# Join the tables using the film_id column\n",
    "film_df_with_ratings = film_df.join(\n",
    "    ratings_per_film_df,\n",
    "    film_df.film_id==ratings_per_film_df.film_id\n",
    ")\n",
    "\n",
    "# Show the 5 first results\n",
    "print(film_df_with_ratings.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen there are two types of databases which are typically used: analytics and application databases.\n",
    "\n",
    "### Analytics\n",
    "\n",
    "- Column-oriented\n",
    "- Optimal for analytics which are queries about a subset of columns, generally\n",
    "- Also lent themselves better to parallelization\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Row-oriented which means we store data per record\n",
    "- Makes it easier to add new rows in small transactions (e.g. getting a customer record is easy and fast)\n",
    "\n",
    "### MPP\n",
    "\n",
    "- Massively Parallel Processing Databases\n",
    "- Often the target at the end of an ETL process\n",
    "- Column-oriented databases optimised for analytics that run in a distributed fashion\n",
    "- Queries are split into subtasks and executed across a cluster of nodes\n",
    "\n",
    "#### Redshift\n",
    "\n",
    "- Example of an MPP\n",
    "- To load data into Redshift a good way is to write files to S3 and then send the copy query to Redshift\n",
    "- `.parquet` is a typical file type to use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple code snippet to load data into Redshift\n",
    "\n",
    "# pandas\n",
    "df.to_parquet(\"./s3://path/tp/bucket/customer.parquet\")\n",
    "# pyspark\n",
    "df.write.parquet(\"./s3://path/tp/bucket/customer.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY customer\n",
    "FROM \"./s3://path/tp/bucket/customer.parquet\"\n",
    "FORMAT as parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation on data\n",
    "recommendations = transform_find_recommendations(ratings_df)\n",
    "\n",
    "# load into postgresql database\n",
    "recommendations.to_sql(\n",
    "    \"recommendations\",\n",
    "    db_engine,\n",
    "    schema=\"store\"\n",
    "    if_exists=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL - Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL should be encapsulted into a clean ETL function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_to_df(tablename, db_engine):\n",
    "    return pd.read_sql(f\"SELECT * FROM {tablename}\", db_engine)\n",
    "\n",
    "def split_columns_transform(df, column, pattern, suffixes):\n",
    "    # converts column in str and splits it on a pattern \n",
    "\n",
    "def load_df_into_dwh(film_df, tablename, schema, db_engine):\n",
    "    return pd.to_sql(tablename, db_engine, schema=schema, if_exists=\"replace\")\n",
    "\n",
    "db_engines = { ... } # needs to be configured\n",
    "\n",
    "def etl():\n",
    "    # Extract\n",
    "    film_df = extract_table_to_df(\"film\", db_engines[\"store\"])\n",
    "    # Transform\n",
    "    film_df = split_columns_transform(film_df, \"rental_rate\", \".\", [\"dollar\", \"_cents\"])\n",
    "    # Load\n",
    "    load_df_into_dwh(film_df, \"film\", \"store\", db_engines[\"dwh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow\n",
    "\n",
    "Recall:\n",
    "\n",
    "- Workflow scheduler\n",
    "- Python\n",
    "- DAGs\n",
    "- Tasks are defined in operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Data on the Command Line\n",
    "\n",
    "## `curl`\n",
    "\n",
    "- short for `c`lient for `url`s\n",
    "- unix command line tool\n",
    "- basic syntax: `curl [options flags] [URL]` - the URL is required\n",
    "\n",
    "Example usage if a file is stored at https://websitename.com/datafilename.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://websitename.com/datafilename.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `curl` flags\n",
    "\n",
    "- `-O`: retains the original filename\n",
    "- `-o`: give the file a new name (need to add the new file name after)\n",
    "- `-L`: redirects the HTTP URL if a 300 error code occurs\n",
    "- `C`: resumes a previous file transfer if it times out before completion\n",
    "\n",
    "### Wildcards in `curl`\n",
    "\n",
    "Often times, servers have files with similar filenames like:\n",
    "- https://websitename.com/datafilename001.txt\n",
    "- https://websitename.com/datafilename002.txt\n",
    "- https://websitename.com/datafilename100.txt\n",
    "\n",
    "We can use wildcards to get them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://websitename.com/datafilename*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or get just file 1 through 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://websitename.com/datafilename[001-100].txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or get just every 10th file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://websitename.com/datafilename[001-100:10].txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1944k  100 1944k    0     0  1767k      0  0:00:01  0:00:01 --:--:-- 1768k\n"
     ]
    }
   ],
   "source": [
    "# Download and rename the file in the same step\n",
    "# -L is needed because it is a tinyurl so a redirect will happen\n",
    "!curl -o ../datasets/Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt --> datafile001.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[2/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile002.txt --> datafile002.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile002.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[3/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile003.txt --> datafile003.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile003.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[4/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile004.txt --> datafile004.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile004.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[5/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile005.txt --> datafile005.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile005.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[6/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile006.txt --> datafile006.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile006.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[7/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile007.txt --> datafile007.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile007.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[8/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile008.txt --> datafile008.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile008.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[9/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile009.txt --> datafile009.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile009.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[10/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile010.txt --> datafile010.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile010.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[11/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile011.txt --> datafile011.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile011.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[12/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile012.txt --> datafile012.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile012.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[13/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile013.txt --> datafile013.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile013.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[14/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile014.txt --> datafile014.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile014.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[15/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile015.txt --> datafile015.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile015.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[16/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile016.txt --> datafile016.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile016.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[17/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile017.txt --> datafile017.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile017.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[18/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile018.txt --> datafile018.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile018.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[19/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile019.txt --> datafile019.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile019.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[20/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile020.txt --> datafile020.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile020.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[21/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile021.txt --> datafile021.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile021.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[22/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile022.txt --> datafile022.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile022.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[23/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile023.txt --> datafile023.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile023.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[24/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile024.txt --> datafile024.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile024.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[25/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile025.txt --> datafile025.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile025.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[26/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile026.txt --> datafile026.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile026.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[27/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile027.txt --> datafile027.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile027.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[28/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile028.txt --> datafile028.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile028.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[29/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile029.txt --> datafile029.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile029.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[30/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile030.txt --> datafile030.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile030.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[31/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile031.txt --> datafile031.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile031.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[32/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile032.txt --> datafile032.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile032.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[33/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile033.txt --> datafile033.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile033.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[34/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile034.txt --> datafile034.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile034.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[35/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile035.txt --> datafile035.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile035.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[36/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile036.txt --> datafile036.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile036.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[37/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile037.txt --> datafile037.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile037.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[38/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile038.txt --> datafile038.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile038.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[39/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile039.txt --> datafile039.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile039.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[40/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile040.txt --> datafile040.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile040.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[41/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile041.txt --> datafile041.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile041.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[42/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile042.txt --> datafile042.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile042.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[43/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile043.txt --> datafile043.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile043.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[44/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile044.txt --> datafile044.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile044.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[45/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile045.txt --> datafile045.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile045.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[46/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile046.txt --> datafile046.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile046.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[47/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile047.txt --> datafile047.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile047.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[48/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile048.txt --> datafile048.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile048.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[49/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile049.txt --> datafile049.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile049.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[50/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile050.txt --> datafile050.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile050.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[51/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile051.txt --> datafile051.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile051.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[52/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile052.txt --> datafile052.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile052.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[53/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile053.txt --> datafile053.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile053.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[54/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile054.txt --> datafile054.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile054.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[55/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile055.txt --> datafile055.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile055.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[56/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile056.txt --> datafile056.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile056.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[57/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile057.txt --> datafile057.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile057.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[58/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile058.txt --> datafile058.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile058.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[59/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile059.txt --> datafile059.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile059.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[60/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile060.txt --> datafile060.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile060.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[61/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile061.txt --> datafile061.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile061.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[62/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile062.txt --> datafile062.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile062.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[63/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile063.txt --> datafile063.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile063.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[64/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile064.txt --> datafile064.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile064.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[65/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile065.txt --> datafile065.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile065.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[66/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile066.txt --> datafile066.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile066.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[67/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile067.txt --> datafile067.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile067.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[68/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile068.txt --> datafile068.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile068.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[69/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile069.txt --> datafile069.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile069.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[70/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile070.txt --> datafile070.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile070.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[71/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile071.txt --> datafile071.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile071.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[72/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile072.txt --> datafile072.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile072.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[73/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile073.txt --> datafile073.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile073.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[74/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile074.txt --> datafile074.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile074.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[75/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile075.txt --> datafile075.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile075.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[76/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile076.txt --> datafile076.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile076.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[77/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile077.txt --> datafile077.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile077.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[78/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile078.txt --> datafile078.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile078.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[79/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile079.txt --> datafile079.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile079.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[80/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile080.txt --> datafile080.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile080.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[81/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile081.txt --> datafile081.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile081.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[82/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile082.txt --> datafile082.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile082.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[83/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile083.txt --> datafile083.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile083.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[84/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile084.txt --> datafile084.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile084.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[85/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile085.txt --> datafile085.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile085.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[86/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile086.txt --> datafile086.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile086.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[87/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile087.txt --> datafile087.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile087.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[88/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile088.txt --> datafile088.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile088.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[89/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile089.txt --> datafile089.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile089.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[90/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile090.txt --> datafile090.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile090.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[91/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile091.txt --> datafile091.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile091.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[92/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile092.txt --> datafile092.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile092.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[93/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile093.txt --> datafile093.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile093.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[94/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile094.txt --> datafile094.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile094.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[95/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile095.txt --> datafile095.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile095.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[96/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile096.txt --> datafile096.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile096.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[97/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile097.txt --> datafile097.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile097.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[98/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile098.txt --> datafile098.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile098.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[99/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile099.txt --> datafile099.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile099.txt\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "[100/100]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile100.txt --> datafile100.txt\n",
      "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile100.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "datafile001.txt datafile021.txt datafile041.txt datafile061.txt datafile081.txt\n",
      "datafile002.txt datafile022.txt datafile042.txt datafile062.txt datafile082.txt\n",
      "datafile003.txt datafile023.txt datafile043.txt datafile063.txt datafile083.txt\n",
      "datafile004.txt datafile024.txt datafile044.txt datafile064.txt datafile084.txt\n",
      "datafile005.txt datafile025.txt datafile045.txt datafile065.txt datafile085.txt\n",
      "datafile006.txt datafile026.txt datafile046.txt datafile066.txt datafile086.txt\n",
      "datafile007.txt datafile027.txt datafile047.txt datafile067.txt datafile087.txt\n",
      "datafile008.txt datafile028.txt datafile048.txt datafile068.txt datafile088.txt\n",
      "datafile009.txt datafile029.txt datafile049.txt datafile069.txt datafile089.txt\n",
      "datafile010.txt datafile030.txt datafile050.txt datafile070.txt datafile090.txt\n",
      "datafile011.txt datafile031.txt datafile051.txt datafile071.txt datafile091.txt\n",
      "datafile012.txt datafile032.txt datafile052.txt datafile072.txt datafile092.txt\n",
      "datafile013.txt datafile033.txt datafile053.txt datafile073.txt datafile093.txt\n",
      "datafile014.txt datafile034.txt datafile054.txt datafile074.txt datafile094.txt\n",
      "datafile015.txt datafile035.txt datafile055.txt datafile075.txt datafile095.txt\n",
      "datafile016.txt datafile036.txt datafile056.txt datafile076.txt datafile096.txt\n",
      "datafile017.txt datafile037.txt datafile057.txt datafile077.txt datafile097.txt\n",
      "datafile018.txt datafile038.txt datafile058.txt datafile078.txt datafile098.txt\n",
      "datafile019.txt datafile039.txt datafile059.txt datafile079.txt datafile099.txt\n",
      "datafile020.txt datafile040.txt datafile060.txt datafile080.txt datafile100.txt\n"
     ]
    }
   ],
   "source": [
    "# Download all 100 data files\n",
    "# saves them with the original filename like datafile001\n",
    "!curl -O https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile[001-100].txt\n",
    "\n",
    "# Print all downloaded files to directory\n",
    "!ls datafile*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `wget`\n",
    "\n",
    "- Another command line tool to get data from the internet\n",
    "- Comes from \"world wide web get\"\n",
    "\n",
    "Check if installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/wget\r\n"
     ]
    }
   ],
   "source": [
    "!which wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic syntax is like `curl`: `wget [option flags] [URL]\n",
    "- Some unique `wget` flags are:\n",
    "    - `-b`: go to background after startup\n",
    "    - `-q`: turn off the `Wget` output\n",
    "    - `-c`: resume a broken download\n",
    "- Flags can be combined like `-bqc`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log file\n",
    "\n",
    "`wget` generates a log file which we can inspect to check everything went ok. The file is called `wget-log`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing in a list of urls from a file\n",
    "\n",
    "`wget` can accept a file with a list of urls to download data from. TO signal we are feeding in urls from a file we use the `-i` flag. \n",
    "\n",
    "> all other flags must appear before the `-i` flag (the filename must appear immediately after the `-i`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure `wget` does not consume full bandwidth with a download\n",
    "\n",
    "We can set an upper download bandwidth limit with `--limit-rate` (whole number automatically converts to kb/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --limit-rate=200k -i url_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Being considerate of the file host server\n",
    "\n",
    "We can also set the download to wait so we are not taxing the file server too much. That is accomplished with `--wait` (default is seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --wait=2 -i url_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `curl` vs `wget`\n",
    "\n",
    "- `curl` advantages:\n",
    "    - can be used to download and upload files from 20+ protocols\n",
    "    - easier to install across all operating systems\n",
    "- `wget` advantages:\n",
    "    - handle multiple file downloads gracefully\n",
    "    - can handle various file formats for download (e.g. file directory, HTML page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of `curl` and `wget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use curl, download and rename a single file from URL\n",
    "!curl -o Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "# Unzip, delete, then re-name to Spotify201812.csv\n",
    "!unzip Spotify201812.zip && rm Spotify201812.zip\n",
    "!mv 201812SpotifyData.csv Spotify201812.csv\n",
    "\n",
    "# View url_list.txt to verify content\n",
    "!cat url_list.txt\n",
    "\n",
    "# Use Wget, limit the download rate to 2500KB/s, download all files in url_list.txt\n",
    "!wget --limit-rate=2500k -i url_list.txt\n",
    "\n",
    "# Take a look at all files downloaded\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Munging on the Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `csvkit`\n",
    "\n",
    "`csvkit` is a suite of command-line tools to process and clean `csv` files in the command-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting csvkit\n",
      "  Downloading csvkit-1.0.4.tar.gz (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting agate>=1.6.1\n",
      "  Downloading agate-1.6.1-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 9.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting agate-excel>=0.2.2\n",
      "  Downloading agate-excel-0.2.3.tar.gz (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 23.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting agate-dbf>=0.2.0\n",
      "  Downloading agate_dbf-0.2.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting agate-sql>=0.5.3\n",
      "  Downloading agate-sql-0.5.4.tar.gz (6.3 kB)\n",
      "Requirement already satisfied: six>=1.6.1 in /Users/miguel.carvalho/.pyenv/versions/3.7.5/envs/myenv/lib/python3.7/site-packages (from csvkit) (1.14.0)\n",
      "Collecting Babel>=2.0\n",
      "  Downloading Babel-2.8.0-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytimeparse>=1.1.5\n",
      "  Downloading pytimeparse-1.1.8-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting python-slugify>=1.2.1\n",
      "  Downloading python-slugify-4.0.0.tar.gz (8.8 kB)\n",
      "Collecting leather>=0.3.2\n",
      "  Downloading leather-0.3.3-py3-none-any.whl (35 kB)\n",
      "Collecting isodate>=0.5.4\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 8.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting parsedatetime>=2.1\n",
      "  Downloading parsedatetime-2.5.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 16.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: xlrd>=0.9.4 in /Users/miguel.carvalho/.pyenv/versions/3.7.5/envs/myenv/lib/python3.7/site-packages (from agate-excel>=0.2.2->csvkit) (1.2.0)\n",
      "Collecting openpyxl>=2.3.0\n",
      "  Downloading openpyxl-3.0.3.tar.gz (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 18.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dbfread>=2.0.5\n",
      "  Downloading dbfread-2.0.7-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.0.8 in /Users/miguel.carvalho/.pyenv/versions/3.7.5/envs/myenv/lib/python3.7/site-packages (from agate-sql>=0.5.3->csvkit) (1.3.8)\n",
      "Requirement already satisfied: pytz>=2015.7 in /Users/miguel.carvalho/.pyenv/versions/3.7.5/envs/myenv/lib/python3.7/site-packages (from Babel>=2.0->agate>=1.6.1->csvkit) (2019.3)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 6.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting jdcal\n",
      "  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting et_xmlfile\n",
      "  Downloading et_xmlfile-1.0.1.tar.gz (8.4 kB)\n",
      "Installing collected packages: Babel, pytimeparse, text-unidecode, python-slugify, leather, isodate, parsedatetime, agate, jdcal, et-xmlfile, openpyxl, agate-excel, dbfread, agate-dbf, agate-sql, csvkit\n",
      "    Running setup.py install for python-slugify ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for parsedatetime ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for et-xmlfile ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for openpyxl ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for agate-excel ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for agate-sql ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for csvkit ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed Babel-2.8.0 agate-1.6.1 agate-dbf-0.2.1 agate-excel-0.2.3 agate-sql-0.5.4 csvkit-1.0.4 dbfread-2.0.7 et-xmlfile-1.0.1 isodate-0.6.0 jdcal-1.4.1 leather-0.3.3 openpyxl-3.0.3 parsedatetime-2.5 python-slugify-4.0.0 pytimeparse-1.1.8 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "# written in python so we install it with pip\n",
    "# the docs are online (not accessible with man)\n",
    "!pip install csvkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common commands\n",
    "\n",
    "- `in2csv` converts an `xlsx` file to `csv`\n",
    "    - `in2csv SpotifyData.xlsx > SpotifyData.csv`\n",
    "    - specifying which sheets to convert\n",
    "        - `in2csv SpotifyData.xlsx --sheet \"Worksheet1_Popularity\" > Spotify_Popularity.csv`\n",
    "    - help with `in2csv -h`\n",
    "\n",
    "> `in2csv` does not print logs to the console so always double check with `ls`\n",
    "\n",
    "- `csvlook` prints csv files in the command line in a formatted way\n",
    "    - `csvlook Spotify_Popularity.csv`\n",
    "    - help with `csvlook -h`\n",
    "- `csvstat` is similar to `pd.describe()`\n",
    "    - `csvstat Spotify_Popularity.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data with `csvkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `csv` can be filtered with:\n",
    "- `csvcut` for columns\n",
    "- `csvgrep` for rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `csvcut`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: track_id\r\n",
      "  2: danceability\r\n",
      "  3: duration_ms\r\n",
      "  4: instrumentalness\r\n",
      "  5: loudness\r\n",
      "  6: tempo\r\n",
      "  7: time_signature\r\n"
     ]
    }
   ],
   "source": [
    "# csvcut can filter by col name or col position\n",
    "# using -n shows us the col names\n",
    "!csvcut -n ../datasets/Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: track_id\r\n",
      "  2: danceability\r\n",
      "  3: duration_ms\r\n",
      "  4: instrumentalness\r\n",
      "  5: loudness\r\n",
      "  6: tempo\r\n",
      "  7: time_signature\r\n"
     ]
    }
   ],
   "source": [
    "# we can use names to be extra explicit\n",
    "!csvcut --names ../datasets/Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id\r\n",
      "118GQ70Sp6pMqn6w1oKuki\r\n",
      "6S7cr72a7a8RVAXzDCRj6m\r\n",
      "7h2qWpMJzIVtiP30E8VDW4\r\n",
      "3KVQFxJ5CWOcbxdpPYdi4o\r\n",
      "0JjNrI1xmsTfhaiU1R6OVc\r\n",
      "3HjTcZt29JUHg5m60QhlMw\r\n",
      "42LWRdkWxM9aWmDImWvH6C\r\n",
      "32dMH9MvlTJaABrPHY52Yb\r\n",
      "5RCPsfzmEpTXMCTNk7wEfQ\r\n",
      "0y0mwXrdEzjUK5Nq8GDPnY\r\n",
      "3RSMqu36JZnmMkrnNmnqyd\r\n",
      "1o0fkWCltFHVeFIRHqvR5b\r\n",
      "2iGShSeV6WcDbez5SLJ2bJ\r\n",
      "2rNTo0tGUMW6rn0uHzV5er\r\n",
      "5Egkx8edirN0pR2R58C2ME\r\n",
      "67r3lnzstENsRYlZWq6DYP\r\n",
      "4X8W9SSu9D5MujoxwIwqw6\r\n",
      "4lncSzeN8WOH2iHEO593iZ\r\n",
      "1L67mcddFQ65MfA3wO3MHV\r\n",
      "21DU83QG4jB4mQKh76X32h\r\n",
      "08nyEVO684j7pcTAhEY2zJ\r\n",
      "4LMVmlX8WXPu8OyPwzkNpR\r\n",
      "7JYCpIzpoidDOnnmxmHwtj\r\n",
      "0mmFibEg5NuULMwTVN2tRU\r\n"
     ]
    }
   ],
   "source": [
    "# Now we can filter more easily (by position)\n",
    "!csvcut -c 1 ../datasets/Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id\r\n",
      "118GQ70Sp6pMqn6w1oKuki\r\n",
      "6S7cr72a7a8RVAXzDCRj6m\r\n",
      "7h2qWpMJzIVtiP30E8VDW4\r\n",
      "3KVQFxJ5CWOcbxdpPYdi4o\r\n",
      "0JjNrI1xmsTfhaiU1R6OVc\r\n",
      "3HjTcZt29JUHg5m60QhlMw\r\n",
      "42LWRdkWxM9aWmDImWvH6C\r\n",
      "32dMH9MvlTJaABrPHY52Yb\r\n",
      "5RCPsfzmEpTXMCTNk7wEfQ\r\n",
      "0y0mwXrdEzjUK5Nq8GDPnY\r\n",
      "3RSMqu36JZnmMkrnNmnqyd\r\n",
      "1o0fkWCltFHVeFIRHqvR5b\r\n",
      "2iGShSeV6WcDbez5SLJ2bJ\r\n",
      "2rNTo0tGUMW6rn0uHzV5er\r\n",
      "5Egkx8edirN0pR2R58C2ME\r\n",
      "67r3lnzstENsRYlZWq6DYP\r\n",
      "4X8W9SSu9D5MujoxwIwqw6\r\n",
      "4lncSzeN8WOH2iHEO593iZ\r\n",
      "1L67mcddFQ65MfA3wO3MHV\r\n",
      "21DU83QG4jB4mQKh76X32h\r\n",
      "08nyEVO684j7pcTAhEY2zJ\r\n",
      "4LMVmlX8WXPu8OyPwzkNpR\r\n",
      "7JYCpIzpoidDOnnmxmHwtj\r\n",
      "0mmFibEg5NuULMwTVN2tRU\r\n"
     ]
    }
   ],
   "source": [
    "# Now we can filter more easily (by name)\n",
    "!csvcut -c \"track_id\" ../datasets/Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id,danceability\n",
      "118GQ70Sp6pMqn6w1oKuki,0.787\n",
      "6S7cr72a7a8RVAXzDCRj6m,0.777\n",
      "7h2qWpMJzIVtiP30E8VDW4,0.795999999999999\n",
      "3KVQFxJ5CWOcbxdpPYdi4o,0.815\n",
      "0JjNrI1xmsTfhaiU1R6OVc,0.799\n",
      "3HjTcZt29JUHg5m60QhlMw,0.812\n",
      "42LWRdkWxM9aWmDImWvH6C,0.810999999999999\n",
      "32dMH9MvlTJaABrPHY52Yb,0.746\n",
      "5RCPsfzmEpTXMCTNk7wEfQ,0.813\n",
      "0y0mwXrdEzjUK5Nq8GDPnY,0.812\n",
      "3RSMqu36JZnmMkrnNmnqyd,0.814\n",
      "1o0fkWCltFHVeFIRHqvR5b,0.813\n",
      "2iGShSeV6WcDbez5SLJ2bJ,0.81\n",
      "2rNTo0tGUMW6rn0uHzV5er,0.805999999999999\n",
      "5Egkx8edirN0pR2R58C2ME,0.812\n",
      "67r3lnzstENsRYlZWq6DYP,0.802\n",
      "4X8W9SSu9D5MujoxwIwqw6,0.822\n",
      "4lncSzeN8WOH2iHEO593iZ,0.809\n",
      "1L67mcddFQ65MfA3wO3MHV,0.805999999999999\n",
      "21DU83QG4jB4mQKh76X32h,0.812\n",
      "08nyEVO684j7pcTAhEY2zJ,0.81\n",
      "4LMVmlX8WXPu8OyPwzkNpR,0.813\n",
      "7JYCpIzpoidDOnnmxmHwtj,0.759\n",
      "0mmFibEg5NuULMwTVN2tRU,0.81\n"
     ]
    }
   ],
   "source": [
    "# Filtering more than one col by position\n",
    "!csvcut -c 1,2 ../datasets/Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id,danceability\r\n",
      "118GQ70Sp6pMqn6w1oKuki,0.787\r\n",
      "6S7cr72a7a8RVAXzDCRj6m,0.777\r\n",
      "7h2qWpMJzIVtiP30E8VDW4,0.795999999999999\r\n",
      "3KVQFxJ5CWOcbxdpPYdi4o,0.815\r\n",
      "0JjNrI1xmsTfhaiU1R6OVc,0.799\r\n",
      "3HjTcZt29JUHg5m60QhlMw,0.812\r\n",
      "42LWRdkWxM9aWmDImWvH6C,0.810999999999999\r\n",
      "32dMH9MvlTJaABrPHY52Yb,0.746\r\n",
      "5RCPsfzmEpTXMCTNk7wEfQ,0.813\r\n",
      "0y0mwXrdEzjUK5Nq8GDPnY,0.812\r\n",
      "3RSMqu36JZnmMkrnNmnqyd,0.814\r\n",
      "1o0fkWCltFHVeFIRHqvR5b,0.813\r\n",
      "2iGShSeV6WcDbez5SLJ2bJ,0.81\r\n",
      "2rNTo0tGUMW6rn0uHzV5er,0.805999999999999\r\n",
      "5Egkx8edirN0pR2R58C2ME,0.812\r\n",
      "67r3lnzstENsRYlZWq6DYP,0.802\r\n",
      "4X8W9SSu9D5MujoxwIwqw6,0.822\r\n",
      "4lncSzeN8WOH2iHEO593iZ,0.809\r\n",
      "1L67mcddFQ65MfA3wO3MHV,0.805999999999999\r\n",
      "21DU83QG4jB4mQKh76X32h,0.812\r\n",
      "08nyEVO684j7pcTAhEY2zJ,0.81\r\n",
      "4LMVmlX8WXPu8OyPwzkNpR,0.813\r\n",
      "7JYCpIzpoidDOnnmxmHwtj,0.759\r\n",
      "0mmFibEg5NuULMwTVN2tRU,0.81\r\n"
     ]
    }
   ],
   "source": [
    "# Filtering more than one col by name\n",
    "# notice no space between the col names\n",
    "!csvcut -c \"track_id\",\"danceability\" ../datasets/Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `csvgrep`\n",
    "\n",
    "- filters by row using exact match or regex fuzzy matching\n",
    "- must be paired with one of these options\n",
    "    - `-m`: followed by the exact row value to filter\n",
    "    - `-r`: followed by a regex pattern\n",
    "    - `-l`: followed by the path to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id,popularity\r\n",
      "4X8W9SSu9D5MujoxwIwqw6,6.0\r\n"
     ]
    }
   ],
   "source": [
    "# get just rows for this track id by name\n",
    "!csvgrep -c \"track_id\" -m 4X8W9SSu9D5MujoxwIwqw6 ../datasets/Spotify_Popularity.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id,popularity\r\n",
      "4X8W9SSu9D5MujoxwIwqw6,6.0\r\n"
     ]
    }
   ],
   "source": [
    "# get just rows for this track id by id\n",
    "!csvgrep -c 1 -m 4X8W9SSu9D5MujoxwIwqw6 ../datasets/Spotify_Popularity.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Column positions are 1-based unlike in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> think of `-c` as in column and `-m` as... m-row?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking data and chaining commands with `csvkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `csvstack` to combine multiple csv files\n",
    "\n",
    "- Similar to `pd.concat`: appends files\n",
    "- Help with `csvstat -h`\n",
    "- Sample usage (assuming same file structure) \n",
    "    - `csvstack Spotify_Rank6.csv Spotify_Rank7.csv > Spotify_AllRanks.csv`\n",
    "- Useful to keep a record of which row came from which file\n",
    "    - `csvstack -g \"Rank6\",\"Rank7\" Spotify_Rank6.csv Spotify_Rank7.csv > Spotify_AllRanks.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files\n",
    "!csvstack ../datasets/Spotify_Popularity.csv ../datasets/Spotify_Popularity_1.csv > SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id,popularity\r\n",
      "118GQ70Sp6pMqn6w1oKuki,7.0\r\n",
      "6S7cr72a7a8RVAXzDCRj6m,7.0\r\n",
      "7h2qWpMJzIVtiP30E8VDW4,7.0\r\n",
      "3KVQFxJ5CWOcbxdpPYdi4o,7.0\r\n",
      "0JjNrI1xmsTfhaiU1R6OVc,7.0\r\n",
      "3HjTcZt29JUHg5m60QhlMw,7.0\r\n",
      "42LWRdkWxM9aWmDImWvH6C,7.0\r\n",
      "32dMH9MvlTJaABrPHY52Yb,7.0\r\n",
      "5RCPsfzmEpTXMCTNk7wEfQ,7.0\r\n",
      "0y0mwXrdEzjUK5Nq8GDPnY,7.0\r\n",
      "3RSMqu36JZnmMkrnNmnqyd,6.0\r\n",
      "1o0fkWCltFHVeFIRHqvR5b,6.0\r\n",
      "2iGShSeV6WcDbez5SLJ2bJ,6.0\r\n",
      "2rNTo0tGUMW6rn0uHzV5er,6.0\r\n",
      "5Egkx8edirN0pR2R58C2ME,6.0\r\n",
      "67r3lnzstENsRYlZWq6DYP,6.0\r\n",
      "4X8W9SSu9D5MujoxwIwqw6,6.0\r\n",
      "4lncSzeN8WOH2iHEO593iZ,6.0\r\n",
      "1L67mcddFQ65MfA3wO3MHV,6.0\r\n",
      "21DU83QG4jB4mQKh76X32h,6.0\r\n",
      "08nyEVO684j7pcTAhEY2zJ,6.0\r\n",
      "4LMVmlX8WXPu8OyPwzkNpR,6.0\r\n",
      "7JYCpIzpoidDOnnmxmHwtj,6.0\r\n",
      "0mmFibEg5NuULMwTVN2tRU,6.0\r\n",
      "118GQ70Sp6pMqn6w1oKuki,7.0\r\n",
      "6S7cr72a7a8RVAXzDCRj6m,7.0\r\n",
      "7h2qWpMJzIVtiP30E8VDW4,7.0\r\n",
      "3KVQFxJ5CWOcbxdpPYdi4o,7.0\r\n",
      "0JjNrI1xmsTfhaiU1R6OVc,7.0\r\n",
      "3HjTcZt29JUHg5m60QhlMw,7.0\r\n",
      "42LWRdkWxM9aWmDImWvH6C,7.0\r\n",
      "32dMH9MvlTJaABrPHY52Yb,7.0\r\n",
      "5RCPsfzmEpTXMCTNk7wEfQ,7.0\r\n",
      "0y0mwXrdEzjUK5Nq8GDPnY,7.0\r\n",
      "3RSMqu36JZnmMkrnNmnqyd,6.0\r\n",
      "1o0fkWCltFHVeFIRHqvR5b,6.0\r\n",
      "2iGShSeV6WcDbez5SLJ2bJ,6.0\r\n",
      "2rNTo0tGUMW6rn0uHzV5er,6.0\r\n",
      "5Egkx8edirN0pR2R58C2ME,6.0\r\n",
      "67r3lnzstENsRYlZWq6DYP,6.0\r\n",
      "4X8W9SSu9D5MujoxwIwqw6,6.0\r\n",
      "4lncSzeN8WOH2iHEO593iZ,6.0\r\n",
      "1L67mcddFQ65MfA3wO3MHV,6.0\r\n",
      "21DU83QG4jB4mQKh76X32h,6.0\r\n",
      "08nyEVO684j7pcTAhEY2zJ,6.0\r\n",
      "4LMVmlX8WXPu8OyPwzkNpR,6.0\r\n",
      "7JYCpIzpoidDOnnmxmHwtj,6.0\r\n",
      "0mmFibEg5NuULMwTVN2tRU,6.0\r\n"
     ]
    }
   ],
   "source": [
    "!cat SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a flag to know which rows came from where\n",
    "!csvstack -g \"File1\",\"File2\" ../datasets/Spotify_Popularity.csv ../datasets/Spotify_Popularity_1.csv > SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group,track_id,popularity\r\n",
      "File1,118GQ70Sp6pMqn6w1oKuki,7.0\r\n",
      "File1,6S7cr72a7a8RVAXzDCRj6m,7.0\r\n",
      "File1,7h2qWpMJzIVtiP30E8VDW4,7.0\r\n",
      "File1,3KVQFxJ5CWOcbxdpPYdi4o,7.0\r\n",
      "File1,0JjNrI1xmsTfhaiU1R6OVc,7.0\r\n",
      "File1,3HjTcZt29JUHg5m60QhlMw,7.0\r\n",
      "File1,42LWRdkWxM9aWmDImWvH6C,7.0\r\n",
      "File1,32dMH9MvlTJaABrPHY52Yb,7.0\r\n",
      "File1,5RCPsfzmEpTXMCTNk7wEfQ,7.0\r\n",
      "File1,0y0mwXrdEzjUK5Nq8GDPnY,7.0\r\n",
      "File1,3RSMqu36JZnmMkrnNmnqyd,6.0\r\n",
      "File1,1o0fkWCltFHVeFIRHqvR5b,6.0\r\n",
      "File1,2iGShSeV6WcDbez5SLJ2bJ,6.0\r\n",
      "File1,2rNTo0tGUMW6rn0uHzV5er,6.0\r\n",
      "File1,5Egkx8edirN0pR2R58C2ME,6.0\r\n",
      "File1,67r3lnzstENsRYlZWq6DYP,6.0\r\n",
      "File1,4X8W9SSu9D5MujoxwIwqw6,6.0\r\n",
      "File1,4lncSzeN8WOH2iHEO593iZ,6.0\r\n",
      "File1,1L67mcddFQ65MfA3wO3MHV,6.0\r\n",
      "File1,21DU83QG4jB4mQKh76X32h,6.0\r\n",
      "File1,08nyEVO684j7pcTAhEY2zJ,6.0\r\n",
      "File1,4LMVmlX8WXPu8OyPwzkNpR,6.0\r\n",
      "File1,7JYCpIzpoidDOnnmxmHwtj,6.0\r\n",
      "File1,0mmFibEg5NuULMwTVN2tRU,6.0\r\n",
      "File2,118GQ70Sp6pMqn6w1oKuki,7.0\r\n",
      "File2,6S7cr72a7a8RVAXzDCRj6m,7.0\r\n",
      "File2,7h2qWpMJzIVtiP30E8VDW4,7.0\r\n",
      "File2,3KVQFxJ5CWOcbxdpPYdi4o,7.0\r\n",
      "File2,0JjNrI1xmsTfhaiU1R6OVc,7.0\r\n",
      "File2,3HjTcZt29JUHg5m60QhlMw,7.0\r\n",
      "File2,42LWRdkWxM9aWmDImWvH6C,7.0\r\n",
      "File2,32dMH9MvlTJaABrPHY52Yb,7.0\r\n",
      "File2,5RCPsfzmEpTXMCTNk7wEfQ,7.0\r\n",
      "File2,0y0mwXrdEzjUK5Nq8GDPnY,7.0\r\n",
      "File2,3RSMqu36JZnmMkrnNmnqyd,6.0\r\n",
      "File2,1o0fkWCltFHVeFIRHqvR5b,6.0\r\n",
      "File2,2iGShSeV6WcDbez5SLJ2bJ,6.0\r\n",
      "File2,2rNTo0tGUMW6rn0uHzV5er,6.0\r\n",
      "File2,5Egkx8edirN0pR2R58C2ME,6.0\r\n",
      "File2,67r3lnzstENsRYlZWq6DYP,6.0\r\n",
      "File2,4X8W9SSu9D5MujoxwIwqw6,6.0\r\n",
      "File2,4lncSzeN8WOH2iHEO593iZ,6.0\r\n",
      "File2,1L67mcddFQ65MfA3wO3MHV,6.0\r\n",
      "File2,21DU83QG4jB4mQKh76X32h,6.0\r\n",
      "File2,08nyEVO684j7pcTAhEY2zJ,6.0\r\n",
      "File2,4LMVmlX8WXPu8OyPwzkNpR,6.0\r\n",
      "File2,7JYCpIzpoidDOnnmxmHwtj,6.0\r\n",
      "File2,0mmFibEg5NuULMwTVN2tRU,6.0\r\n"
     ]
    }
   ],
   "source": [
    "!cat SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a flag to know which rows came from where and changing col \n",
    "# name from default group to something else\n",
    "!csvstack -g \"File1\",\"File2\" -n \"source\" ../datasets/Spotify_Popularity.csv ../datasets/Spotify_Popularity_1.csv > SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source,track_id,popularity\r\n",
      "File1,118GQ70Sp6pMqn6w1oKuki,7.0\r\n",
      "File1,6S7cr72a7a8RVAXzDCRj6m,7.0\r\n",
      "File1,7h2qWpMJzIVtiP30E8VDW4,7.0\r\n",
      "File1,3KVQFxJ5CWOcbxdpPYdi4o,7.0\r\n",
      "File1,0JjNrI1xmsTfhaiU1R6OVc,7.0\r\n",
      "File1,3HjTcZt29JUHg5m60QhlMw,7.0\r\n",
      "File1,42LWRdkWxM9aWmDImWvH6C,7.0\r\n",
      "File1,32dMH9MvlTJaABrPHY52Yb,7.0\r\n",
      "File1,5RCPsfzmEpTXMCTNk7wEfQ,7.0\r\n",
      "File1,0y0mwXrdEzjUK5Nq8GDPnY,7.0\r\n",
      "File1,3RSMqu36JZnmMkrnNmnqyd,6.0\r\n",
      "File1,1o0fkWCltFHVeFIRHqvR5b,6.0\r\n",
      "File1,2iGShSeV6WcDbez5SLJ2bJ,6.0\r\n",
      "File1,2rNTo0tGUMW6rn0uHzV5er,6.0\r\n",
      "File1,5Egkx8edirN0pR2R58C2ME,6.0\r\n",
      "File1,67r3lnzstENsRYlZWq6DYP,6.0\r\n",
      "File1,4X8W9SSu9D5MujoxwIwqw6,6.0\r\n",
      "File1,4lncSzeN8WOH2iHEO593iZ,6.0\r\n",
      "File1,1L67mcddFQ65MfA3wO3MHV,6.0\r\n",
      "File1,21DU83QG4jB4mQKh76X32h,6.0\r\n",
      "File1,08nyEVO684j7pcTAhEY2zJ,6.0\r\n",
      "File1,4LMVmlX8WXPu8OyPwzkNpR,6.0\r\n",
      "File1,7JYCpIzpoidDOnnmxmHwtj,6.0\r\n",
      "File1,0mmFibEg5NuULMwTVN2tRU,6.0\r\n",
      "File2,118GQ70Sp6pMqn6w1oKuki,7.0\r\n",
      "File2,6S7cr72a7a8RVAXzDCRj6m,7.0\r\n",
      "File2,7h2qWpMJzIVtiP30E8VDW4,7.0\r\n",
      "File2,3KVQFxJ5CWOcbxdpPYdi4o,7.0\r\n",
      "File2,0JjNrI1xmsTfhaiU1R6OVc,7.0\r\n",
      "File2,3HjTcZt29JUHg5m60QhlMw,7.0\r\n",
      "File2,42LWRdkWxM9aWmDImWvH6C,7.0\r\n",
      "File2,32dMH9MvlTJaABrPHY52Yb,7.0\r\n",
      "File2,5RCPsfzmEpTXMCTNk7wEfQ,7.0\r\n",
      "File2,0y0mwXrdEzjUK5Nq8GDPnY,7.0\r\n",
      "File2,3RSMqu36JZnmMkrnNmnqyd,6.0\r\n",
      "File2,1o0fkWCltFHVeFIRHqvR5b,6.0\r\n",
      "File2,2iGShSeV6WcDbez5SLJ2bJ,6.0\r\n",
      "File2,2rNTo0tGUMW6rn0uHzV5er,6.0\r\n",
      "File2,5Egkx8edirN0pR2R58C2ME,6.0\r\n",
      "File2,67r3lnzstENsRYlZWq6DYP,6.0\r\n",
      "File2,4X8W9SSu9D5MujoxwIwqw6,6.0\r\n",
      "File2,4lncSzeN8WOH2iHEO593iZ,6.0\r\n",
      "File2,1L67mcddFQ65MfA3wO3MHV,6.0\r\n",
      "File2,21DU83QG4jB4mQKh76X32h,6.0\r\n",
      "File2,08nyEVO684j7pcTAhEY2zJ,6.0\r\n",
      "File2,4LMVmlX8WXPu8OyPwzkNpR,6.0\r\n",
      "File2,7JYCpIzpoidDOnnmxmHwtj,6.0\r\n",
      "File2,0mmFibEg5NuULMwTVN2tRU,6.0\r\n"
     ]
    }
   ],
   "source": [
    "!cat SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining command-line commands\n",
    "\n",
    "- `;` links commands together and runs sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| source | track_id               | popularity |\n",
      "| ------ | ---------------------- | ---------- |\n",
      "| File1  | 118GQ70Sp6pMqn6w1oKuki |          7 |\n",
      "| File1  | 6S7cr72a7a8RVAXzDCRj6m |          7 |\n",
      "| File1  | 7h2qWpMJzIVtiP30E8VDW4 |          7 |\n",
      "| File1  | 3KVQFxJ5CWOcbxdpPYdi4o |          7 |\n",
      "| File1  | 0JjNrI1xmsTfhaiU1R6OVc |          7 |\n",
      "| File1  | 3HjTcZt29JUHg5m60QhlMw |          7 |\n",
      "| File1  | 42LWRdkWxM9aWmDImWvH6C |          7 |\n",
      "| File1  | 32dMH9MvlTJaABrPHY52Yb |          7 |\n",
      "| File1  | 5RCPsfzmEpTXMCTNk7wEfQ |          7 |\n",
      "| File1  | 0y0mwXrdEzjUK5Nq8GDPnY |          7 |\n",
      "| File1  | 3RSMqu36JZnmMkrnNmnqyd |          6 |\n",
      "| File1  | 1o0fkWCltFHVeFIRHqvR5b |          6 |\n",
      "| File1  | 2iGShSeV6WcDbez5SLJ2bJ |          6 |\n",
      "| File1  | 2rNTo0tGUMW6rn0uHzV5er |          6 |\n",
      "| File1  | 5Egkx8edirN0pR2R58C2ME |          6 |\n",
      "| File1  | 67r3lnzstENsRYlZWq6DYP |          6 |\n",
      "| File1  | 4X8W9SSu9D5MujoxwIwqw6 |          6 |\n",
      "| File1  | 4lncSzeN8WOH2iHEO593iZ |          6 |\n",
      "| File1  | 1L67mcddFQ65MfA3wO3MHV |          6 |\n",
      "| File1  | 21DU83QG4jB4mQKh76X32h |          6 |\n",
      "| File1  | 08nyEVO684j7pcTAhEY2zJ |          6 |\n",
      "| File1  | 4LMVmlX8WXPu8OyPwzkNpR |          6 |\n",
      "| File1  | 7JYCpIzpoidDOnnmxmHwtj |          6 |\n",
      "| File1  | 0mmFibEg5NuULMwTVN2tRU |          6 |\n",
      "| File2  | 118GQ70Sp6pMqn6w1oKuki |          7 |\n",
      "| File2  | 6S7cr72a7a8RVAXzDCRj6m |          7 |\n",
      "| File2  | 7h2qWpMJzIVtiP30E8VDW4 |          7 |\n",
      "| File2  | 3KVQFxJ5CWOcbxdpPYdi4o |          7 |\n",
      "| File2  | 0JjNrI1xmsTfhaiU1R6OVc |          7 |\n",
      "| File2  | 3HjTcZt29JUHg5m60QhlMw |          7 |\n",
      "| File2  | 42LWRdkWxM9aWmDImWvH6C |          7 |\n",
      "| File2  | 32dMH9MvlTJaABrPHY52Yb |          7 |\n",
      "| File2  | 5RCPsfzmEpTXMCTNk7wEfQ |          7 |\n",
      "| File2  | 0y0mwXrdEzjUK5Nq8GDPnY |          7 |\n",
      "| File2  | 3RSMqu36JZnmMkrnNmnqyd |          6 |\n",
      "| File2  | 1o0fkWCltFHVeFIRHqvR5b |          6 |\n",
      "| File2  | 2iGShSeV6WcDbez5SLJ2bJ |          6 |\n",
      "| File2  | 2rNTo0tGUMW6rn0uHzV5er |          6 |\n",
      "| File2  | 5Egkx8edirN0pR2R58C2ME |          6 |\n",
      "| File2  | 67r3lnzstENsRYlZWq6DYP |          6 |\n",
      "| File2  | 4X8W9SSu9D5MujoxwIwqw6 |          6 |\n",
      "| File2  | 4lncSzeN8WOH2iHEO593iZ |          6 |\n",
      "| File2  | 1L67mcddFQ65MfA3wO3MHV |          6 |\n",
      "| File2  | 21DU83QG4jB4mQKh76X32h |          6 |\n",
      "| File2  | 08nyEVO684j7pcTAhEY2zJ |          6 |\n",
      "| File2  | 4LMVmlX8WXPu8OyPwzkNpR |          6 |\n",
      "| File2  | 7JYCpIzpoidDOnnmxmHwtj |          6 |\n",
      "| File2  | 0mmFibEg5NuULMwTVN2tRU |          6 |\n",
      "  1. \"source\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         2\n",
      "\tLongest value:         5 characters\n",
      "\tMost common values:    File1 (24x)\n",
      "\t                       File2 (24x)\n",
      "\n",
      "  2. \"track_id\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         24\n",
      "\tLongest value:         22 characters\n",
      "\tMost common values:    118GQ70Sp6pMqn6w1oKuki (2x)\n",
      "\t                       6S7cr72a7a8RVAXzDCRj6m (2x)\n",
      "\t                       7h2qWpMJzIVtiP30E8VDW4 (2x)\n",
      "\t                       3KVQFxJ5CWOcbxdpPYdi4o (2x)\n",
      "\t                       0JjNrI1xmsTfhaiU1R6OVc (2x)\n",
      "\n",
      "  3. \"popularity\"\n",
      "\n",
      "\tType of data:          Number\n",
      "\tContains null values:  False\n",
      "\tUnique values:         2\n",
      "\tSmallest value:        6\n",
      "\tLargest value:         7\n",
      "\tSum:                   308\n",
      "\tMean:                  6.417\n",
      "\tMedian:                6\n",
      "\tStDev:                 0.498\n",
      "\tMost common values:    6 (28x)\n",
      "\t                       7 (20x)\n",
      "\n",
      "Row count: 48\n"
     ]
    }
   ],
   "source": [
    "!csvlook SpotifyCopy.csv; csvstat SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `&&` links commands together but second runs only if the first one succeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| source | track_id               | popularity |\n",
      "| ------ | ---------------------- | ---------- |\n",
      "| File1  | 118GQ70Sp6pMqn6w1oKuki |          7 |\n",
      "| File1  | 6S7cr72a7a8RVAXzDCRj6m |          7 |\n",
      "| File1  | 7h2qWpMJzIVtiP30E8VDW4 |          7 |\n",
      "| File1  | 3KVQFxJ5CWOcbxdpPYdi4o |          7 |\n",
      "| File1  | 0JjNrI1xmsTfhaiU1R6OVc |          7 |\n",
      "| File1  | 3HjTcZt29JUHg5m60QhlMw |          7 |\n",
      "| File1  | 42LWRdkWxM9aWmDImWvH6C |          7 |\n",
      "| File1  | 32dMH9MvlTJaABrPHY52Yb |          7 |\n",
      "| File1  | 5RCPsfzmEpTXMCTNk7wEfQ |          7 |\n",
      "| File1  | 0y0mwXrdEzjUK5Nq8GDPnY |          7 |\n",
      "| File1  | 3RSMqu36JZnmMkrnNmnqyd |          6 |\n",
      "| File1  | 1o0fkWCltFHVeFIRHqvR5b |          6 |\n",
      "| File1  | 2iGShSeV6WcDbez5SLJ2bJ |          6 |\n",
      "| File1  | 2rNTo0tGUMW6rn0uHzV5er |          6 |\n",
      "| File1  | 5Egkx8edirN0pR2R58C2ME |          6 |\n",
      "| File1  | 67r3lnzstENsRYlZWq6DYP |          6 |\n",
      "| File1  | 4X8W9SSu9D5MujoxwIwqw6 |          6 |\n",
      "| File1  | 4lncSzeN8WOH2iHEO593iZ |          6 |\n",
      "| File1  | 1L67mcddFQ65MfA3wO3MHV |          6 |\n",
      "| File1  | 21DU83QG4jB4mQKh76X32h |          6 |\n",
      "| File1  | 08nyEVO684j7pcTAhEY2zJ |          6 |\n",
      "| File1  | 4LMVmlX8WXPu8OyPwzkNpR |          6 |\n",
      "| File1  | 7JYCpIzpoidDOnnmxmHwtj |          6 |\n",
      "| File1  | 0mmFibEg5NuULMwTVN2tRU |          6 |\n",
      "| File2  | 118GQ70Sp6pMqn6w1oKuki |          7 |\n",
      "| File2  | 6S7cr72a7a8RVAXzDCRj6m |          7 |\n",
      "| File2  | 7h2qWpMJzIVtiP30E8VDW4 |          7 |\n",
      "| File2  | 3KVQFxJ5CWOcbxdpPYdi4o |          7 |\n",
      "| File2  | 0JjNrI1xmsTfhaiU1R6OVc |          7 |\n",
      "| File2  | 3HjTcZt29JUHg5m60QhlMw |          7 |\n",
      "| File2  | 42LWRdkWxM9aWmDImWvH6C |          7 |\n",
      "| File2  | 32dMH9MvlTJaABrPHY52Yb |          7 |\n",
      "| File2  | 5RCPsfzmEpTXMCTNk7wEfQ |          7 |\n",
      "| File2  | 0y0mwXrdEzjUK5Nq8GDPnY |          7 |\n",
      "| File2  | 3RSMqu36JZnmMkrnNmnqyd |          6 |\n",
      "| File2  | 1o0fkWCltFHVeFIRHqvR5b |          6 |\n",
      "| File2  | 2iGShSeV6WcDbez5SLJ2bJ |          6 |\n",
      "| File2  | 2rNTo0tGUMW6rn0uHzV5er |          6 |\n",
      "| File2  | 5Egkx8edirN0pR2R58C2ME |          6 |\n",
      "| File2  | 67r3lnzstENsRYlZWq6DYP |          6 |\n",
      "| File2  | 4X8W9SSu9D5MujoxwIwqw6 |          6 |\n",
      "| File2  | 4lncSzeN8WOH2iHEO593iZ |          6 |\n",
      "| File2  | 1L67mcddFQ65MfA3wO3MHV |          6 |\n",
      "| File2  | 21DU83QG4jB4mQKh76X32h |          6 |\n",
      "| File2  | 08nyEVO684j7pcTAhEY2zJ |          6 |\n",
      "| File2  | 4LMVmlX8WXPu8OyPwzkNpR |          6 |\n",
      "| File2  | 7JYCpIzpoidDOnnmxmHwtj |          6 |\n",
      "| File2  | 0mmFibEg5NuULMwTVN2tRU |          6 |\n",
      "  1. \"source\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         2\n",
      "\tLongest value:         5 characters\n",
      "\tMost common values:    File1 (24x)\n",
      "\t                       File2 (24x)\n",
      "\n",
      "  2. \"track_id\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         24\n",
      "\tLongest value:         22 characters\n",
      "\tMost common values:    118GQ70Sp6pMqn6w1oKuki (2x)\n",
      "\t                       6S7cr72a7a8RVAXzDCRj6m (2x)\n",
      "\t                       7h2qWpMJzIVtiP30E8VDW4 (2x)\n",
      "\t                       3KVQFxJ5CWOcbxdpPYdi4o (2x)\n",
      "\t                       0JjNrI1xmsTfhaiU1R6OVc (2x)\n",
      "\n",
      "  3. \"popularity\"\n",
      "\n",
      "\tType of data:          Number\n",
      "\tContains null values:  False\n",
      "\tUnique values:         2\n",
      "\tSmallest value:        6\n",
      "\tLargest value:         7\n",
      "\tSum:                   308\n",
      "\tMean:                  6.417\n",
      "\tMedian:                6\n",
      "\tStDev:                 0.498\n",
      "\tMost common values:    6 (28x)\n",
      "\t                       7 (20x)\n",
      "\n",
      "Row count: 48\n"
     ]
    }
   ],
   "source": [
    "!csvlook SpotifyCopy.csv && csvstat SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `>` redirects output \n",
    "- `|` uses the output of the 1st command as input to the second command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id,danceability\r\n",
      "118GQ70Sp6pMqn6w1oKuki,0.787\r\n",
      "6S7cr72a7a8RVAXzDCRj6m,0.777\r\n",
      "7h2qWpMJzIVtiP30E8VDW4,0.795999999999999\r\n",
      "3KVQFxJ5CWOcbxdpPYdi4o,0.815\r\n",
      "0JjNrI1xmsTfhaiU1R6OVc,0.799\r\n",
      "3HjTcZt29JUHg5m60QhlMw,0.812\r\n",
      "42LWRdkWxM9aWmDImWvH6C,0.810999999999999\r\n",
      "32dMH9MvlTJaABrPHY52Yb,0.746\r\n",
      "5RCPsfzmEpTXMCTNk7wEfQ,0.813\r\n",
      "0y0mwXrdEzjUK5Nq8GDPnY,0.812\r\n",
      "3RSMqu36JZnmMkrnNmnqyd,0.814\r\n",
      "1o0fkWCltFHVeFIRHqvR5b,0.813\r\n",
      "2iGShSeV6WcDbez5SLJ2bJ,0.81\r\n",
      "2rNTo0tGUMW6rn0uHzV5er,0.805999999999999\r\n",
      "5Egkx8edirN0pR2R58C2ME,0.812\r\n",
      "67r3lnzstENsRYlZWq6DYP,0.802\r\n",
      "4X8W9SSu9D5MujoxwIwqw6,0.822\r\n",
      "4lncSzeN8WOH2iHEO593iZ,0.809\r\n",
      "1L67mcddFQ65MfA3wO3MHV,0.805999999999999\r\n",
      "21DU83QG4jB4mQKh76X32h,0.812\r\n",
      "08nyEVO684j7pcTAhEY2zJ,0.81\r\n",
      "4LMVmlX8WXPu8OyPwzkNpR,0.813\r\n",
      "7JYCpIzpoidDOnnmxmHwtj,0.759\r\n",
      "0mmFibEg5NuULMwTVN2tRU,0.81\r\n"
     ]
    }
   ],
   "source": [
    "# works but the format sucks, we can pipe it to csvlook\n",
    "!csvcut -c \"track_id\",\"danceability\" ../datasets/Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| track_id               | danceability |\r\n",
      "| ---------------------- | ------------ |\r\n",
      "| 118GQ70Sp6pMqn6w1oKuki |       0.787… |\r\n",
      "| 6S7cr72a7a8RVAXzDCRj6m |       0.777… |\r\n",
      "| 7h2qWpMJzIVtiP30E8VDW4 |       0.796… |\r\n",
      "| 3KVQFxJ5CWOcbxdpPYdi4o |       0.815… |\r\n",
      "| 0JjNrI1xmsTfhaiU1R6OVc |       0.799… |\r\n",
      "| 3HjTcZt29JUHg5m60QhlMw |       0.812… |\r\n",
      "| 42LWRdkWxM9aWmDImWvH6C |       0.811… |\r\n",
      "| 32dMH9MvlTJaABrPHY52Yb |       0.746… |\r\n",
      "| 5RCPsfzmEpTXMCTNk7wEfQ |       0.813… |\r\n",
      "| 0y0mwXrdEzjUK5Nq8GDPnY |       0.812… |\r\n",
      "| 3RSMqu36JZnmMkrnNmnqyd |       0.814… |\r\n",
      "| 1o0fkWCltFHVeFIRHqvR5b |       0.813… |\r\n",
      "| 2iGShSeV6WcDbez5SLJ2bJ |       0.810… |\r\n",
      "| 2rNTo0tGUMW6rn0uHzV5er |       0.806… |\r\n",
      "| 5Egkx8edirN0pR2R58C2ME |       0.812… |\r\n",
      "| 67r3lnzstENsRYlZWq6DYP |       0.802… |\r\n",
      "| 4X8W9SSu9D5MujoxwIwqw6 |       0.822… |\r\n",
      "| 4lncSzeN8WOH2iHEO593iZ |       0.809… |\r\n",
      "| 1L67mcddFQ65MfA3wO3MHV |       0.806… |\r\n",
      "| 21DU83QG4jB4mQKh76X32h |       0.812… |\r\n",
      "| 08nyEVO684j7pcTAhEY2zJ |       0.810… |\r\n",
      "| 4LMVmlX8WXPu8OyPwzkNpR |       0.813… |\r\n",
      "| 7JYCpIzpoidDOnnmxmHwtj |       0.759… |\r\n",
      "| 0mmFibEg5NuULMwTVN2tRU |       0.810… |\r\n"
     ]
    }
   ],
   "source": [
    "# better\n",
    "!csvcut -c \"track_id\",\"danceability\" ../datasets/Spotify_MusicAttributes.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Operations on the Command Line\n",
    "\n",
    "`sql2csv` is a command in the `csvkit` library which allows us to access data on a variety of SQL databases. It executes the command and saves it into a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sql2csv --db \"sqlite:///SpotifyDatabase.db\" \\\n",
    "         --query \"SELECT * FROM Spotify_Popularity\" \\\n",
    "         > Spotify_popularity.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `--db` is followed by the database connection string\n",
    "    - Examples:\n",
    "        - `sqlite:///` (and ends with `.db`)\n",
    "        - `postgres:///` or `mysql///`\n",
    "    - Is a string so needs to be in quotation marks\n",
    "\n",
    "- `--query` is followed by the actual query\n",
    "- Is a string so needs to be in quotation marks\n",
    "- **The query needs to be written in one single line with no breaks**\n",
    "\n",
    "We then redirect the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful flags\n",
    "\n",
    "We can also add `--verbose` if we are getting an error and want to see the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sqlite3.OperationalError) no such table: Spotify_Popularity\r\n",
      "[SQL: SELECT * FROM Spotify_Popularity]\r\n",
      "(Background on this error at: http://sqlalche.me/e/e3q8)\r\n"
     ]
    }
   ],
   "source": [
    "# the table is not available locally\n",
    "! sql2csv --db \"sqlite:///SpotifyDatabase.db\" \\\n",
    "        --query \"SELECT * FROM Spotify_Popularity\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data using SQL syntax\n",
    "\n",
    "We can use SQL to inspect csv files with `csvkit`: it convert the csv file to a temporary SQL database under the hood. This is possible with `csvsql`\n",
    "\n",
    "> This is not suitable for large file processing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source,track_id,popularity\r\n",
      "File1,118GQ70Sp6pMqn6w1oKuki,7.0\r\n"
     ]
    }
   ],
   "source": [
    "# We're using a SQL query ona csv file\n",
    "!csvsql --query \"SELECT * FROM SpotifyCopy LIMIT 1\" SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| source | track_id               | popularity |\r\n",
      "| ------ | ---------------------- | ---------- |\r\n",
      "| File1  | 118GQ70Sp6pMqn6w1oKuki |          7 |\r\n"
     ]
    }
   ],
   "source": [
    "# The output looks better if we pipe it\n",
    "!csvsql --query \"SELECT * FROM SpotifyCopy LIMIT 1\" SpotifyCopy.csv | \\\n",
    "    csvlook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Files\n",
    "\n",
    "We can join files too using `csvsql`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your file is not \"utf-8\" encoded. Please specify the correct encoding with the -e flag or with the PYTHONIOENCODING environment variable. Use the -v flag to see the complete error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!csvsql --query \"SELECT * FROM SpotifyCopy INNER JOIN SpotifyCopy\" SpotifyCopy.csv ../datasets/Spotify201812.zip \\\n",
    "    | csvlook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving queries as variables\n",
    "\n",
    "Sometimes the query can get long so it's more readable to save it as a shell variable and then pass it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_data_processing_in_shell.ipynb datafile050.txt\n",
      "SpotifyCopy.csv                  datafile051.txt\n",
      "SpotifyDatabase.db               datafile052.txt\n",
      "datafile001.txt                  datafile053.txt\n",
      "datafile002.txt                  datafile054.txt\n",
      "datafile003.txt                  datafile055.txt\n",
      "datafile004.txt                  datafile056.txt\n",
      "datafile005.txt                  datafile057.txt\n",
      "datafile006.txt                  datafile058.txt\n",
      "datafile007.txt                  datafile059.txt\n",
      "datafile008.txt                  datafile060.txt\n",
      "datafile009.txt                  datafile061.txt\n",
      "datafile010.txt                  datafile062.txt\n",
      "datafile011.txt                  datafile063.txt\n",
      "datafile012.txt                  datafile064.txt\n",
      "datafile013.txt                  datafile065.txt\n",
      "datafile014.txt                  datafile066.txt\n",
      "datafile015.txt                  datafile067.txt\n",
      "datafile016.txt                  datafile068.txt\n",
      "datafile017.txt                  datafile069.txt\n",
      "datafile018.txt                  datafile070.txt\n",
      "datafile019.txt                  datafile071.txt\n",
      "datafile020.txt                  datafile072.txt\n",
      "datafile021.txt                  datafile073.txt\n",
      "datafile022.txt                  datafile074.txt\n",
      "datafile023.txt                  datafile075.txt\n",
      "datafile024.txt                  datafile076.txt\n",
      "datafile025.txt                  datafile077.txt\n",
      "datafile026.txt                  datafile078.txt\n",
      "datafile027.txt                  datafile079.txt\n",
      "datafile028.txt                  datafile080.txt\n",
      "datafile029.txt                  datafile081.txt\n",
      "datafile030.txt                  datafile082.txt\n",
      "datafile031.txt                  datafile083.txt\n",
      "datafile032.txt                  datafile084.txt\n",
      "datafile033.txt                  datafile085.txt\n",
      "datafile034.txt                  datafile086.txt\n",
      "datafile035.txt                  datafile087.txt\n",
      "datafile036.txt                  datafile088.txt\n",
      "datafile037.txt                  datafile089.txt\n",
      "datafile038.txt                  datafile090.txt\n",
      "datafile039.txt                  datafile091.txt\n",
      "datafile040.txt                  datafile092.txt\n",
      "datafile041.txt                  datafile093.txt\n",
      "datafile042.txt                  datafile094.txt\n",
      "datafile043.txt                  datafile095.txt\n",
      "datafile044.txt                  datafile096.txt\n",
      "datafile045.txt                  datafile097.txt\n",
      "datafile046.txt                  datafile098.txt\n",
      "datafile047.txt                  datafile099.txt\n",
      "datafile048.txt                  datafile100.txt\n",
      "datafile049.txt\n",
      "CREATE TABLE \"SpotifyCopy\" (\n",
      "\tsource VARCHAR NOT NULL, \n",
      "\ttrack_id VARCHAR NOT NULL, \n",
      "\tpopularity DECIMAL NOT NULL\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "# Preview CSV file\n",
    "!ls\n",
    "\n",
    "# Store SQL query as shell variable\n",
    "!sqlquery=\"SELECT * FROM SpotifyCopy ORDER BY duration_ms LIMIT 1\"\n",
    "\n",
    "# Apply SQL query to Spotify_MusicAttributes.csv\n",
    "# output gets a bit weird in jupyter\n",
    "!csvsql --query \"$sqlquery\" SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing data back to the database\n",
    "\n",
    "Using `csvsql` we can:\n",
    "- execute SQL statements directly on a database (`--query`)\n",
    "- create and insert data (`--insert` and `--db`)\n",
    "\n",
    "Sample syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates an empty table, specifies schema, and inserts rows\n",
    "!csvsql --db \"sqlite:///SpotifyDatabase.db\" --insert SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --no-inference for do not assume filetypes\n",
    "# --no-constraints for generating a schema without length limits or null checks\n",
    "# helpful for large tables, speeds up the process\n",
    "!csvsql --no-inference --no-constraints --db \"sqlite:///SpotifyDatabase_2.db\" --insert SpotifyCopy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline on the command line\n",
    "\n",
    "## cron\n",
    "\n",
    "cron is a scheduler. We can use it to launch jobs with some periodicity.\n",
    "\n",
    "### crontab\n",
    "\n",
    "Jobs are tracked in the `crontab` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crontab: no crontab for miguel.carvalho\r\n"
     ]
    }
   ],
   "source": [
    "!crontab -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CRONTAB(1)                BSD General Commands Manual               CRONTAB(1)\n",
      "\n",
      "NAME\n",
      "     crontab -- maintain crontab files for individual users (V3)\n",
      "\n",
      "SYNOPSIS\n",
      "     crontab [-u user] file\n",
      "     crontab [-u user] { -l | -r | -e }\n",
      "\n",
      "DESCRIPTION\n",
      "     The crontab utility is the program used to install, deinstall or list the\n",
      "     tables used to drive the cron(8) daemon in Vixie Cron.  Each user can\n",
      "     have their own crontab, and they are not intended to be edited directly.\n",
      "\n",
      "     (Darwin note: Although cron(8) and crontab(5) are officially supported\n",
      "     under Darwin, their functionality has been absorbed into launchd(8),\n",
      "     which provides a more flexible way of automatically executing commands.\n",
      "     See launchctl(1) for more information.)\n",
      "\n",
      "     If the /usr/lib/cron/cron.allow file exists, then you must be listed\n",
      "     therein in order to be allowed to use this command.  If the\n",
      "     /usr/lib/cron/cron.allow file does not exist but the\n",
      "     /usr/lib/cron/cron.deny file does exist, then you must not be listed in\n",
      "     the /usr/lib/cron/cron.deny file in order to use this command.  If nei-\n",
      "     ther of these files exists, then depending on site-dependent configura-\n",
      "     tion parameters, only the super user will be allowed to use this command,\n",
      "     or all users will be able to use this command.  The format of these files\n",
      "     is one username per line, with no leading or trailing whitespace.  Lines\n",
      "     of other formats will be ignored, and so can be used for comments.\n",
      "\n",
      "     The first form of this command is used to install a new crontab from some\n",
      "     named file or standard input if the pseudo-filename `-' is given.\n",
      "\n",
      "     The following options are available:\n",
      "\n",
      "     -u      Specify the name of the user whose crontab is to be tweaked.  If\n",
      "             this option is not given, crontab examines ``your'' crontab,\n",
      "             i.e., the crontab of the person executing the command.  Note that\n",
      "             su(1) can confuse crontab and that if you are running inside of\n",
      "             su(1) you should always use the -u option for safety's sake.\n",
      "\n",
      "     -l      Display the current crontab on standard output.\n",
      "\n",
      "     -r      Remove the current crontab.\n",
      "\n",
      "     -e      Edit the current crontab using the editor specified by the VISUAL\n",
      "             or EDITOR environment variables.  The specified editor must edit\n",
      "             the file in place; any editor that unlinks the file and recreates\n",
      "             it cannot be used.  After you exit from the editor, the modified\n",
      "             crontab will be installed automatically.\n",
      "\n",
      "FILES\n",
      "     /usr/lib/cron/cron.allow\n",
      "     /usr/lib/cron/cron.deny\n",
      "\n",
      "DIAGNOSTICS\n",
      "     A fairly informative usage message appears if you run it with a bad com-\n",
      "     mand line.\n",
      "\n",
      "SEE ALSO\n",
      "     crontab(5), compat(5), cron(8), launchctl(1)\n",
      "\n",
      "STANDARDS\n",
      "     The crontab command conforms to IEEE Std 1003.2 (``POSIX.2'').  The new\n",
      "     command syntax differs from previous versions of Vixie Cron, as well as\n",
      "     from the classic SVR3 syntax.\n",
      "\n",
      "AUTHORS\n",
      "     Paul Vixie <paul@vix.com>\n",
      "\n",
      "BSD                            December 29, 1993                           BSD\n"
     ]
    }
   ],
   "source": [
    "!man crontab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding jobs to `crontab`\n",
    "\n",
    "Two alternatives:\n",
    "1. Modify the `crontab` file with `nano`\n",
    "2. `echo` the schedule command into `crontab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run bc I don't want to schedule a job\n",
    "# !echo \"* * * * * python creaete_model.py\" | crontab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `crontab` syntax\n",
    "\n",
    "Check https://crontab.guru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
